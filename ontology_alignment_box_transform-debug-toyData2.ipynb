{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty {\n",
       "      padding: 0;\n",
       "      border: 0;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from boxes import *\n",
    "from learner import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wandb\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在计算一对alignment（A， B）的时候，先找A的父亲A1，A2，A3， A的孩子a1，a2，a3，\n",
    "A的上界是A1,2,3的交集，记为unionBox，A的下界是a1,2,3的并集，记为joinBox\n",
    "joinBox.z < A.z <unionBox.z\n",
    "joinBox.Z < A.Z <unionBox.Z\n",
    "1)将unionBox和joinBox和自己box输入一层weight中去学一个box\n",
    "2)将unionBox和joinBox和自己box作为三个数据，loss相加\n",
    "A1,2,3 a1,2,3 的gradient等于A的gradient乘以他们自己的gradient 这样AB学好之后 A123，a123的gradient也是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"../data/ontologies/anatomy/\"\n",
    "\n",
    "# with open(f'{PATH}human.pickle', 'rb') as handle:\n",
    "#     human = pickle.load(handle)\n",
    "    \n",
    "# with open(f'{PATH}mouse.pickle', 'rb') as handle:\n",
    "#     mouse = pickle.load(handle)\n",
    "\n",
    "# with open(f'{PATH}entities.pickle', 'rb') as handle:\n",
    "#     entities = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# children_dict = {\n",
    "#     1:[2],\n",
    "#     2:[3, 4],\n",
    "#     3:[6],\n",
    "#     4:[5, 6],\n",
    "#     6:[7, 8, 9],\n",
    "#     10:[11, 12],\n",
    "#     11:[13],\n",
    "#     12:[14],\n",
    "#     13:[15, 16],\n",
    "#     14:[15, 16],\n",
    "#     15:[17, 18, 19],\n",
    "#     5:[5],\n",
    "#     7:[7],\n",
    "#     8:[8],\n",
    "#     9:[9],\n",
    "#     16:[16],\n",
    "#     17:[17],\n",
    "#     18:[18],\n",
    "#     19:[19]\n",
    "# }\n",
    "children_dict = {\n",
    "    0:[0, 0, 0],\n",
    "    1:[2, 2, 2],\n",
    "    2:[3, 4, 4],\n",
    "    3:[6, 6, 6],\n",
    "    4:[5, 6, 6],\n",
    "    6:[7, 8, 9],\n",
    "    10:[11, 12, 12],\n",
    "    11:[13, 13, 13],\n",
    "    12:[14, 14, 14],\n",
    "    13:[15, 16, 16],\n",
    "    14:[15, 16, 16],\n",
    "    15:[17, 18, 19],\n",
    "    5:[5, 5, 5],\n",
    "    7:[7, 7, 7],\n",
    "    8:[8, 8, 8],\n",
    "    9:[9, 9, 9],\n",
    "    16:[16, 16, 16],\n",
    "    17:[17, 17, 17],\n",
    "    18:[18, 18, 18],\n",
    "    19:[19, 19, 19]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parents_dict = {\n",
    "#     7:[6],\n",
    "#     8:[6],\n",
    "#     9:[6],\n",
    "#     5:[4],\n",
    "#     6:[3, 4],\n",
    "#     3:[2],\n",
    "#     4:[2],\n",
    "#     2:[1],\n",
    "#     17:[15],\n",
    "#     18:[15],\n",
    "#     19:[15],\n",
    "#     15:[13, 14],\n",
    "#     16:[13, 14],\n",
    "#     13:[11],\n",
    "#     14:[12],\n",
    "#     11:[10],\n",
    "#     12:[10],\n",
    "#     1:[1],\n",
    "#     10:[10]\n",
    "# }\n",
    "parents_dict = {\n",
    "    0:[0, 0, 0],\n",
    "    7:[6, 6, 6],\n",
    "    8:[6, 6, 6],\n",
    "    9:[6, 6, 6],\n",
    "    5:[4, 4, 4],\n",
    "    6:[3, 4, 4],\n",
    "    3:[2, 2, 2],\n",
    "    4:[2, 2, 2],\n",
    "    2:[1, 1, 1],\n",
    "    17:[15, 15, 15],\n",
    "    18:[15, 15, 15],\n",
    "    19:[15, 15, 15],\n",
    "    15:[13, 14, 14],\n",
    "    16:[13, 14, 14],\n",
    "    13:[11, 11, 11],\n",
    "    14:[12, 12, 12],\n",
    "    11:[10, 10, 10],\n",
    "    12:[10, 10, 10],\n",
    "    1:[1, 1, 1],\n",
    "    10:[10, 10, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 0, 0],\n",
       " 7: [6, 6, 6],\n",
       " 8: [6, 6, 6],\n",
       " 9: [6, 6, 6],\n",
       " 5: [4, 4, 4],\n",
       " 6: [3, 4, 4],\n",
       " 3: [2, 2, 2],\n",
       " 4: [2, 2, 2],\n",
       " 2: [1, 1, 1],\n",
       " 17: [15, 15, 15],\n",
       " 18: [15, 15, 15],\n",
       " 19: [15, 15, 15],\n",
       " 15: [13, 14, 14],\n",
       " 16: [13, 14, 14],\n",
       " 13: [11, 11, 11],\n",
       " 14: [12, 12, 12],\n",
       " 11: [10, 10, 10],\n",
       " 12: [10, 10, 10],\n",
       " 1: [1, 1, 1],\n",
       " 10: [10, 10, 10]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed_all(seed)  # gpu\n",
    "    \n",
    "set_seed(54321)\n",
    "\n",
    "torch.set_printoptions(precision=16)\n",
    "\n",
    "split_num = 9\n",
    "weight_alignment_loss = 1\n",
    "Threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/ontologies/anatomy/debug/'\n",
    "\n",
    "# aligment training split\n",
    "ats = \"\"\n",
    "\n",
    "# Transitive closure\n",
    "Transitive_Closure = False\n",
    "\n",
    "if Transitive_Closure:\n",
    "    tc = \"tc_\"\n",
    "else:\n",
    "    tc = \"\"\n",
    "\n",
    "# Data in unary.tsv are probabilites separated by newlines. The probability on line n is P(n), where n is the id assigned to the nth element.\n",
    "unary_prob = [1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "#unary_prob = [1/3, 1/3, 1/3, 1/3, 1/3, 1/3, 1/3]\n",
    "unary_prob = torch.tensor(unary_prob).float().to(device)\n",
    "num_boxes = unary_prob.shape[0]\n",
    "\n",
    "# We're going to use random negative sampling during training, so no need to include negatives in our training data itself\n",
    "train = Probs.load_from_julia(PATH, f'tr_pos-Copy1.tsv', f'tr_neg-Copy1.tsv', ratio_neg = 7).to(device)\n",
    "\n",
    "# The dev set will have a fixed set of negatives, however.\n",
    "dev = Probs.load_from_julia(PATH, f'dev_align_pos-Copy1.tsv', f'dev_align_neg-Copy1.tsv', ratio_neg = 4).to(device)\n",
    "\n",
    "# This set is used just for evaluation purposes after training\n",
    "tr_align = Probs.load_from_julia(PATH, f'tr_align_pos-Copy1.tsv', f'tr_align_neg-Copy1.tsv', ratio_neg = 0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_eval = Probs.load_from_julia(PATH, 'human_dev_pos-Copy1.tsv', 'human_dev_neg-Copy1.tsv', ratio_neg = 0).to(device)\n",
    "human_eval = Probs.load_from_julia(PATH, 'mouse_dev_pos-Copy1.tsv', 'mouse_dev_neg-Copy1.tsv', ratio_neg = 0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def test_multi_intersection(A: Tensor) -> Tensor:\n",
    "#     \"\"\"\n",
    "#     :param A: Tensor(..., zZ, dim)\n",
    "#     :return: Tensor(..., zZ, dim)\n",
    "#     \"\"\"\n",
    "#     print(\"A:\", A.shape)\n",
    "#     z = torch.max(A[...,0,:], dim=2).values\n",
    "    \n",
    "#     print(\"z:\", z.shape)\n",
    "    \n",
    "#     Z = torch.min(A[...,0,:], dim=2).values\n",
    "    \n",
    "#     print(\"result:\", torch.stack((z, Z), dim=-2).shape)\n",
    "#     return torch.stack((z, Z), dim=-2)\n",
    "\n",
    "# test_multi_intersection(torch.randn(1, 28, 3, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor # for type annotations\n",
    "import torch.nn.functional as F\n",
    "from typing import *\n",
    "\n",
    "def multi_intersection(A: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param A: Tensor(..., zZ, dim)\n",
    "    :return: Tensor(..., zZ, dim)\n",
    "    \"\"\"\n",
    "\n",
    "    z = torch.max(A[...,0,:], dim=2).values\n",
    "    Z = torch.min(A[...,1,:], dim=2).values\n",
    "    \n",
    "    return torch.stack((z, Z), dim=-2)\n",
    "\n",
    "def multi_join(A: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param A: Tensor(model, pair, zZ, dim)\n",
    "    :return: Tensor(model, pair, zZ, dim)\n",
    "    \"\"\"\n",
    "    z = torch.min(A[...,0,:], dim=2).values\n",
    "    Z = torch.max(A[...,1,:], dim=2).values\n",
    "    return torch.stack((z, Z), dim=-2)\n",
    "\n",
    "\n",
    "def gumbel_intersection(A: Tensor, B: Tensor, gumbel_beta: float=1.)-> Tensor:\n",
    "    Az = A[...,0,:]\n",
    "    AZ = A[...,1,:]\n",
    "    Bz = B[...,0,:]\n",
    "    BZ = B[...,1,:]\n",
    "    \n",
    "    gumbel_beta = 0.1\n",
    "    z = gumbel_beta * torch.logsumexp(torch.stack((Az / gumbel_beta, Bz / gumbel_beta)), 0)\n",
    "    z = torch.max(z, torch.max(Az, Bz))\n",
    "    Z = - gumbel_beta * torch.logsumexp(torch.stack((-AZ / gumbel_beta, -BZ / gumbel_beta)), 0)\n",
    "    Z = torch.min(Z, torch.min(AZ, BZ))\n",
    "    return torch.stack((z, Z), dim=-2)\n",
    "\n",
    "\n",
    "def log_soft_volume_adjusted(\n",
    "                     boxes: Tensor,\n",
    "                     temp: float = 1.,\n",
    "                     gumbel_beta: float = 1.,\n",
    "                     scale: Union[float, Tensor] = 1.) -> Tensor:\n",
    "    \n",
    "    euler_gamma = 0.57721566490153286060\n",
    "    gumbel_beta = 0.1\n",
    "    \n",
    "    z = boxes[:,:,0]\n",
    "    Z = boxes[:,:,1]\n",
    "    eps = torch.finfo(z.dtype).tiny  # type: ignore\n",
    "    \n",
    "\n",
    "    \n",
    "    if isinstance(scale, float):\n",
    "        s = torch.tensor(scale)\n",
    "    else:\n",
    "        s = scale\n",
    "\n",
    "    return (torch.sum(\n",
    "        torch.log(F.softplus(Z - z - 2*euler_gamma*gumbel_beta, beta=temp).clamp_min(eps)),\n",
    "        dim=-1) + torch.log(s)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def intersection(A: Tensor, B: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param A: Tensor(..., zZ, dim)\n",
    "    :param B: Tensor(..., zZ, dim)\n",
    "    :return: Tensor(..., zZ, dim), box embeddings for A intersect B\n",
    "    \"\"\"\n",
    "    z = torch.max(A[...,0,:], B[...,0,:])\n",
    "    Z = torch.min(A[...,1,:], B[...,1,:])\n",
    "    \n",
    "#     z = gumbel_beta * torch.logsumexp(torch.stack((t1.z / gumbel_beta, t2.z / gumbel_beta)), 0)\n",
    "#     z = torch.max(z, torch.max(t1.z, t2.z))\n",
    "#     Z = - gumbel_beta * torch.logsumexp(torch.stack((-t1.Z / gumbel_beta, -t2.Z / gumbel_beta)), 0)\n",
    "#     Z = torch.min(Z, torch.min(t1.Z, t2.Z))\n",
    "    return torch.stack((z, Z), dim=-2)\n",
    "\n",
    "def neg_edge_adjustment(A: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param A: Tensor(..., zZ, dim)\n",
    "    \n",
    "    Replace \"negative\" edges with their mean.\n",
    "    \n",
    "    (TODO: optimize this)\n",
    "    \"\"\"\n",
    "    center_of_meet = torch.mean(A, dim=-2)\n",
    "    neg_edges_mask = ((A[...,1,:] - A[...,0,:]) >= 0)\n",
    "    neg_edges_mask_stack = torch.stack((neg_edges_mask, neg_edges_mask), dim=-2)\n",
    "    center_of_meet_stack = torch.stack((center_of_meet, center_of_meet), dim=-2)\n",
    "    return torch.where(neg_edges_mask_stack, A, center_of_meet_stack)\n",
    "\n",
    "def join(A: Tensor, B: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param A: Tensor(model, pair, zZ, dim)\n",
    "    :param B: Tensor(model, pair, zZ, dim)\n",
    "    :return: Tensor(model, pair, zZ, dim), box embeddings for the smallest box which contains A and B\n",
    "    \"\"\"\n",
    "    z = torch.min(A[:,:,0], B[:,:,0])\n",
    "    Z = torch.max(A[:,:,1], B[:,:,1])\n",
    "    return torch.stack((z, Z), dim=2)\n",
    "\n",
    "\n",
    "def clamp_volume(boxes: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param boxes: Tensor(... zZ, dim)\n",
    "    :return: Tensor(...) of volumes\n",
    "    \"\"\"\n",
    "    return torch.prod((boxes[...,1,:] - boxes[...,0,:]).clamp_min(0), dim=-1)\n",
    "\n",
    "\n",
    "def soft_volume(boxes: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param sidelengths: Tensor(model, box, dim)\n",
    "    :return: Tensor(model, box) of volumes\n",
    "    \"\"\"\n",
    "#     print(\"you are in soft_volume now!!!\")\n",
    "#     print(\"Z:\",boxes[:,0,1])\n",
    "#     print(\"z:\",boxes[:,0,0])\n",
    "#     print(\"boxes[:,0,1] - boxes[:,0,0]:\", boxes[:,0,1] - boxes[:,0,0])\n",
    "#     print(\"after softplus:\", F.softplus(boxes[:,0,1] - boxes[:,0,0]))\n",
    "    return torch.prod(F.softplus(boxes[:,:,1] - boxes[:,:,0]), dim=-1)\n",
    "\n",
    "\n",
    "def log_clamp_volume(boxes: Tensor, eps:float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param boxes: Tensor(model, box, zZ, dim)\n",
    "    :return: Tensor(model, box) of volumes\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.log((boxes[:,:,1] - boxes[:,:,0]).clamp_min(0) + eps), dim=-1)\n",
    "\n",
    "\n",
    "def log_soft_volume(boxes: Tensor, eps:float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param sidelengths: Tensor(model, box, dim)\n",
    "    :return: Tensor(model, box) of volumes\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.log(F.softplus(boxes[:,:,1] - boxes[:,:,0]) + eps), dim=-1)\n",
    "\n",
    "\n",
    "def smallest_containing_box(boxes: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns the smallest box which contains all boxes in `boxes`.\n",
    "    \n",
    "    :param boxes: Box embedding of shape (model, box, zZ, dim)\n",
    "    :return: Tensor of shape (model, 1, zZ, dim)\n",
    "    \"\"\"\n",
    "    z = boxes[:,:,0]\n",
    "    Z = boxes[:,:,1]\n",
    "    min_z, _ = torch.min(z, dim=1, keepdim=True)\n",
    "    max_Z, _ = torch.max(Z, dim=1, keepdim=True)\n",
    "    return torch.stack((min_z, max_Z), dim=2)\n",
    "\n",
    "def smallest_containing_box_outside_unit_cube(boxes: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns the smallest box which contains all boxes in `boxes` and the unit cube.\n",
    "\n",
    "    :param boxes: Box embedding of shape (model, box, zZ, dim)\n",
    "    :return: Tensor of shape (model, 1, zZ, dim)\n",
    "    \"\"\"\n",
    "    z = boxes[:,:,0]\n",
    "    Z = boxes[:,:,1]\n",
    "    min_z, _ = torch.min(z, dim=1, keepdim=True)\n",
    "    max_Z, _ = torch.max(Z, dim=1, keepdim=True)\n",
    "    min_z = min_z.clamp_max(0)\n",
    "    max_Z = max_Z.clamp_min(1)\n",
    "    return torch.stack((min_z, max_Z), dim=2)\n",
    "\n",
    "\n",
    "def detect_small_boxes(boxes: Tensor, vol_func: Callable = clamp_volume, min_vol: float = 1e-20) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns the indices of boxes with volume smaller than eps.\n",
    "\n",
    "    :param boxes: box parametrization as Tensor(model, box, z/Z, dim)\n",
    "    :param vol_func: function taking in side lengths and returning volumes\n",
    "    :param min_vol: minimum volume of boxes\n",
    "    :return: masked tensor which selects boxes whose side lengths are less than min_vol\n",
    "    \"\"\"\n",
    "    return vol_func(boxes) < min_vol\n",
    "\n",
    "\n",
    "def replace_Z_by_cube(boxes: Tensor, indices: Tensor, cube_vol: float = 1e-20) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a new Z parameter for boxes for which those boxes[indices] are now replaced by cubes of size cube_vol\n",
    "\n",
    "    :param boxes: box parametrization as Tensor(model, box, z/Z, dim)\n",
    "    :param indices: box indices to replace by a cube\n",
    "    :param cube_vol: volume of cube\n",
    "    :return: tensor representing the Z parameter\n",
    "    \"\"\"\n",
    "    return boxes[:, :, 0][indices] + cube_vol ** (1 / boxes.shape[-1])\n",
    "\n",
    "\n",
    "\n",
    "def replace_Z_by_cube_(boxes: Tensor, indices: Tensor, cube_vol: float = 1e-20) -> Tensor:\n",
    "    \"\"\"\n",
    "    Replaces the boxes indexed by `indices` by a cube of volume `min_vol` with the same z coordinate\n",
    "\n",
    "    :param boxes: box parametrization as Tensor(model, box, z/Z, dim)\n",
    "    :param indices: box indices to replace by a cube\n",
    "    :param cube_vol: volume of cube\n",
    "    :return: tensor representing the box parametrization with those boxes\n",
    "    \"\"\"\n",
    "    boxes[:, :, 1][indices] = replace_Z_by_cube(boxes, indices, cube_vol)\n",
    "\n",
    "\n",
    "def disjoint_boxes_mask(A: Tensor, B: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a mask for when A and B are disjoint.\n",
    "    Note: This is symmetric with respect to the arguments.\n",
    "    \"\"\"\n",
    "    return ((B[:,:,1] <= A[:,:,0]) | (B[:,:,0] >= A[:,:,1])).any(dim=-1)\n",
    "\n",
    "\n",
    "def overlapping_boxes_mask(A: Tensor, B: Tensor) -> Tensor:\n",
    "    return disjoint_boxes_mask(A, B) ^ 1\n",
    "\n",
    "\n",
    "def containing_boxes_mask(A: Tensor, B: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a mask for when B contains A.\n",
    "    Note: This is *not* symmetric with respect to it's arguments!\n",
    "    \"\"\"\n",
    "    return ((B[:,:,1] >= A[:,:,1]) & (B[:,:,0] <= A[:,:,0])).all(dim=-1)\n",
    "\n",
    "\n",
    "def needing_pull_mask(A: Tensor, B: Tensor, target_prob_B_given_A: Tensor) -> Tensor:\n",
    "    return (target_prob_B_given_A != 0) & disjoint_boxes_mask(A, B)\n",
    "\n",
    "\n",
    "def needing_push_mask(A: Tensor, B: Tensor, target_prob_B_given_A: Tensor) -> Tensor:\n",
    "    return (target_prob_B_given_A != 1) & containing_boxes_mask(A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import scipy.stats as spstats # For Spearman r\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve  # for roc_curve\n",
    "\n",
    "\n",
    "def metric_hard_accuracy(model, data_in, data_out):\n",
    "    hard_pred = model(data_in, is_align=torch.tensor(0))[\"P(A|B)_evaluate\"] > Threshold\n",
    "#     print(\"data_in:\", data_in)\n",
    "#     print(\"hard_pred:\", hard_pred)\n",
    "    return (data_out == hard_pred.float()).float().mean()\n",
    "\n",
    "\n",
    "def metric_hard_f1(model, data_in, data_out):\n",
    "    hard_pred = model(data_in, is_align=torch.tensor(0))[\"P(A|B)_evaluate\"] > Threshold\n",
    "    true_pos = data_out[hard_pred==1].sum()\n",
    "    total_pred_pos = (hard_pred==1).sum().float()\n",
    "    total_actual_pos = data_out.sum().float()\n",
    "    precision = true_pos / total_pred_pos\n",
    "    recall = true_pos / total_actual_pos\n",
    "    return 2 * (precision*recall) / (precision + recall)\n",
    "\n",
    "def metric_hard_accuracy_align(model, data_in, data_out, threshold:float):\n",
    "    A_given_B = data_in[::2]\n",
    "    B_given_A = data_in[1::2,:]\n",
    "    data_out = data_out[::2]\n",
    "    \n",
    "\n",
    "    align_probs = torch.stack((model(A_given_B, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"], model(B_given_A, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"]), dim=1)\n",
    "    p = torch.min(align_probs, dim=1).values\n",
    "    hard_pred = p > threshold\n",
    "    \n",
    "    print(\"A_given_B:\", A_given_B, model(A_given_B, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"])\n",
    "    print(\"B_given_A:\", B_given_A, model(B_given_A, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"])\n",
    "#     print(\"data_out:\", data_out)\n",
    "    \n",
    "#     print(\"metric_hard_accuracy_align:\", p, threshold, (data_out == hard_pred).float().mean())\n",
    "\n",
    "    return (data_out == hard_pred).float().mean()\n",
    "\n",
    "def metric_hard_f1_align(model, data_in, data_out, threshold:float):\n",
    "    A_given_B = data_in[::2]\n",
    "    B_given_A = data_in[1::2,:]\n",
    "    data_out = data_out[::2]\n",
    "\n",
    "    align_probs = torch.stack((model(A_given_B, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"], model(B_given_A, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"]), dim=1)\n",
    "    p = torch.min(align_probs, dim=1).values\n",
    "    hard_pred = p > threshold\n",
    "\n",
    "    true_pos = data_out[hard_pred==1].sum()\n",
    "    total_pred_pos = (hard_pred==1).sum().float()\n",
    "    total_actual_pos = data_out.sum().float()\n",
    "\n",
    "    precision = true_pos / total_pred_pos\n",
    "    recall = true_pos / total_actual_pos\n",
    "\n",
    "    return 2 * (precision*recall) / (precision + recall)\n",
    "\n",
    "def metric_hard_accuracy_align_mean(model, data_in, data_out, threshold):\n",
    "    A_given_B = data_in[::2]\n",
    "    B_given_A = data_in[1::2,:]\n",
    "    data_out = data_out[::2]\n",
    "\n",
    "    align_probs = torch.stack((model(A_given_B, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"], model(B_given_A, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"]), dim=1)\n",
    "    p = torch.mean(align_probs, dim=1)\n",
    "    hard_pred = p > threshold\n",
    "\n",
    "    return (data_out == hard_pred).float().mean()\n",
    "\n",
    "def metric_hard_f1_align_mean(model, data_in, data_out, threshold):\n",
    "    A_given_B = data_in[::2]\n",
    "    B_given_A = data_in[1::2,:]\n",
    "    data_out = data_out[::2]\n",
    "\n",
    "    align_probs = torch.stack((model(A_given_B, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"], model(B_given_A, is_align=torch.tensor(1))[\"P(A|B)_evaluate\"]), dim=1)\n",
    "    p = torch.mean(align_probs, dim=1)\n",
    "    hard_pred = p > threshold\n",
    "\n",
    "    true_pos = data_out[hard_pred==1].sum()\n",
    "    total_pred_pos = (hard_pred==1).sum().float()\n",
    "    total_actual_pos = data_out.sum().float()\n",
    "\n",
    "    precision = true_pos / total_pred_pos\n",
    "    recall = true_pos / total_actual_pos\n",
    "\n",
    "    return 2 * (precision*recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import *\n",
    "from dataclasses import dataclass, field\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n",
    "        raise ImportError(\"console\")\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.core.display import HTML, display\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from learner import Learner, Recorder\n",
    "\n",
    "\n",
    "class Callback:\n",
    "    def learner_post_init(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def train_begin(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def epoch_begin(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def batch_begin(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def backward_end(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def batch_end(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def epoch_end(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def train_end(self, learner: Learner):\n",
    "        pass\n",
    "\n",
    "    def eval_align(self, learner: Learner, threshold:float):\n",
    "        pass\n",
    "\n",
    "    def metric_plots(self, l: Learner):\n",
    "        pass\n",
    "\n",
    "    def eval_end(self, l: Learner):\n",
    "        pass\n",
    "\n",
    "    def bias_metric(self, l: Learner):\n",
    "        pass\n",
    "\n",
    "class CallbackCollection:\n",
    "\n",
    "    def __init__(self, *callbacks: Callback):\n",
    "        self._callbacks = callbacks\n",
    "\n",
    "    def __call__(self, action: str, *args, **kwargs):\n",
    "        for c in self._callbacks:\n",
    "            getattr(c, action)(*args, **kwargs)\n",
    "\n",
    "    def __getattr__(self, action: str):\n",
    "        return lambda *args, **kwargs: self.__call__(action, *args, **kwargs)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GradientClipping(Callback):\n",
    "    min: float = None\n",
    "    max: float = None\n",
    "\n",
    "    def backward_end(self, learner: Learner):\n",
    "        for param in learner.model.parameters():\n",
    "            if param.grad is not None:\n",
    "#                 print(\"grad:\", param.grad)\n",
    "                param.grad = param.grad.clamp(self.min, self.max)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LossCallback(Callback):\n",
    "    recorder: Recorder\n",
    "    ds: Dataset\n",
    "    weighted: bool = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train_begin(self, learner: Learner):\n",
    "        self.epoch_end(learner)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def epoch_end(self, l: Learner):\n",
    "        data_in, data_out = self.ds[:]\n",
    "        if l.categories:\n",
    "            split_in, split_out = l.split_data(data_in, data_out, split=2737)\n",
    "            \n",
    "            model_pred = []\n",
    "            count = 0\n",
    "            for item in split_in:\n",
    "                if len(item)>0:\n",
    "                    if count<2:\n",
    "                        model_pred.append(l.model(item, is_align=torch.tensor(0)))\n",
    "                    else:\n",
    "                        model_pred.append(l.model(item, is_align=torch.tensor(1)))\n",
    "                else:\n",
    "                    model_pred.append({'P(A|B)':l.TensorNaN(device=data_in.device)})\n",
    "                count+=1\n",
    "\n",
    "            #model_pred = [l.model(item) if len(item)>0 else {'P(A|B)':l.TensorNaN(device=data_in.device)} for item in split_in]\n",
    "            l.loss_fn(model_pred, split_out, l, self.recorder, weighted=self.weighted, categories=True)  \n",
    "        else:\n",
    "            output = l.model(data_in, is_align=torch.tensor(0))\n",
    "            l.loss_fn(output, data_out, l, self.recorder, weighted=self.weighted) # this logs the data to the recorder\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MetricCallback(Callback):\n",
    "    recorder: Recorder\n",
    "    ds: Dataset\n",
    "    data_categories: str\n",
    "    metric: Callable\n",
    "    use_wandb: bool = False\n",
    "    name: Union[str, None] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.name is None:\n",
    "            self.name = self.metric.__name__\n",
    "        self.name = self.recorder.get_unique_name(self.name)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train_begin(self, learner: Learner):\n",
    "        self.epoch_end(learner)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def epoch_end(self, l: Learner):\n",
    "        data_in, data_out = self.ds[:]\n",
    "        metric_val = self.metric(l.model, data_in, data_out)\n",
    "        self.recorder.update_({self.name: metric_val}, l.progress.current_epoch_iter)\n",
    "        \n",
    "        print(\"evaluation_\" + self.data_categories + \"_\" + self.name, str(metric_val))\n",
    "        \n",
    "        if self.use_wandb:\n",
    "            metric_name = \"evaluation_\" + self.data_categories + \"_\" + self.name\n",
    "            wandb.log({metric_name: metric_val})\n",
    "\n",
    "@dataclass\n",
    "class EvalAlignment(Callback):\n",
    "    recorder: Recorder\n",
    "    ds: Dataset\n",
    "    data_categories: str\n",
    "    metric: callable\n",
    "    use_wandb: bool = False\n",
    "    name: Union[str, None] = None\n",
    "        \n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.name is None:\n",
    "            self.name = self.metric.__name__\n",
    "        self.name = self.recorder.get_unique_name(self.name)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_align(self, l: Learner, threshold: float):\n",
    "        data_in, data_out = self.ds[:]\n",
    "        metric_val = self.metric(l.model, data_in, data_out, threshold)\n",
    "        self.recorder.update_({self.name: metric_val}, threshold)\n",
    "        \n",
    "        print(\"align_evaluation_\" + self.data_categories + \"_\" + str(threshold) + \"_\" + self.name, str(metric_val))\n",
    "        \n",
    "        if self.use_wandb:\n",
    "            metric_name = \"align_evaluation_\" + self.data_categories + \"_\" + str(threshold) + \"_\" + self.name\n",
    "            wandb.log({metric_name: metric_val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "################################################\n",
    "# Box Parametrization Layers\n",
    "################################################\n",
    "default_init_min_vol = torch.finfo(torch.float32).tiny\n",
    "\n",
    "def initialize_boxes_in_unit_cube(shape_prefix: Tuple[int], dims:int, init_min_vol: float = default_init_min_vol,\n",
    "    method = \"gibbs\", gibbs_iter: int = 2000, **kwargs):\n",
    "    \"\"\"\n",
    "    Creates the Parameters used for the representation of boxes.\n",
    "    Initializes boxes with a uniformly random distribution of coordinates, ensuring that each box\n",
    "    contains a cube of volume larger than init_min_vol.\n",
    "\n",
    "    :param shape_prefix: Tuple which prefixes the parameters, eg. (num_boxes, num_models) will lead to\n",
    "        a Tensor of shape (num_boxes, num_models, 2, dims)\n",
    "    :param dims: Dimension of boxes\n",
    "    :param init_min_vol: Minimum volume for boxes which are created\n",
    "    :param kwargs: Unused for now, but include this for future possible parameters.\n",
    "    \"\"\"\n",
    "    if method == \"gibbs\":\n",
    "        sides = torch.ones(*shape_prefix, dims)\n",
    "        log_min_vol = torch.log(torch.tensor(init_min_vol))\n",
    "        for i in range(gibbs_iter):\n",
    "            idx = torch.randint(0, dims, shape_prefix)[...,None]\n",
    "            sides.scatter_(-1, idx, 1)\n",
    "            complement = torch.log(sides).sum(dim=-1)\n",
    "            min = torch.exp(log_min_vol - complement)[..., None]\n",
    "            new_lengths = min + torch.rand(idx.shape) * (1 - min)\n",
    "            sides.scatter_(-1, idx, new_lengths)\n",
    "\n",
    "        z = torch.rand(*shape_prefix, dims) * (1-sides)\n",
    "        Z = z + sides\n",
    "\n",
    "    else:\n",
    "        rand_param = lambda min, max: min + torch.rand(*shape_prefix, dims) * (max - min)\n",
    "        if init_min_vol == 0:\n",
    "            per_dim_min = 0\n",
    "        elif init_min_vol > 0:\n",
    "            per_dim_min = torch.tensor(init_min_vol).pow(1/dims)\n",
    "        else:\n",
    "            raise ValueError(f\"init_min_vol={init_min_vol} is an invalid option.\")\n",
    "\n",
    "        z = rand_param(0, 1-per_dim_min)\n",
    "        Z = rand_param(z+per_dim_min, 1)\n",
    "\n",
    "    return torch.stack((z,Z), dim=-2)\n",
    "\n",
    "\n",
    "class BoxParam(Module):\n",
    "    \"\"\"\n",
    "    An example class for creating a box parametrization.\n",
    "    Don't inherit from this, it is just an example which contains the methods for a class to be used as a BoxParam\n",
    "    layer. Refer to the docstring of the functions when implementing your own BoxParam.\n",
    "\n",
    "    Note: to avoid naming conflicts with min/max functions, we refer to the min coordinate for a box as `z`, and the\n",
    "    max coordinate as `Z`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_models:int, num_boxes:int, dim:int, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates the Parameters used for the representation of boxes.\n",
    "\n",
    "        :param num_models: Number of models\n",
    "        :param num_boxes: Number of boxes\n",
    "        :param dim: Dimension\n",
    "        :param kwargs: Unused for now, but include this for future possible parameters.\n",
    "        \"\"\"\n",
    "        # Remember to call:\n",
    "        super().__init__()\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "    def forward(self, box_indices = slice(None, None, None), **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a Tensor representing the boxes specified by `box_indices` in the form they should be used for training.\n",
    "\n",
    "        :param box_indices: Slice, List, or Tensor of the box indices\n",
    "        :param kwargs: Unused for now, but include this for future possible parameters.\n",
    "        :return: Tensor of shape (model, id, zZ, dim).\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Boxes(Module):\n",
    "    \"\"\"\n",
    "    Parametrize boxes using the min coordinate and max coordinate,\n",
    "    initialized to be in the unit hypercube.\n",
    "\n",
    "    self.boxes[model, box, min/max, dim] \\in [0,1]\n",
    "\n",
    "    In this parametrization, the min and max coordinates are explicitly stored\n",
    "    in separate dimensions (as shown above), which means that care must be\n",
    "    taken to preserve max > min while training. (See MinBoxSize Callback.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_models: int, num_boxes: int, dims: int,\n",
    "                 init_min_vol: float = default_init_min_vol, method = \"gibbs\", gibbs_iter: int = 2000, **kwargs):\n",
    "        super().__init__()\n",
    "        self.boxes = Parameter(initialize_boxes_in_unit_cube((num_models, num_boxes), dims, init_min_vol, method, gibbs_iter, **kwargs))\n",
    "\n",
    "    def forward(self, box_indices = slice(None, None, None), **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a Tensor representing the box embeddings specified by box_indices.\n",
    "\n",
    "        :param box_indices: Slice, List, or Tensor of the box indices\n",
    "        :param kwargs: Unused for now, but include this for future possible parameters.\n",
    "        :return: NamedTensor of shape (model, id, zZ, dim).\n",
    "        \"\"\"\n",
    "        return self.boxes[:, box_indices]\n",
    "\n",
    "\n",
    "\n",
    "class MinMaxSigmoidBoxes(Module):\n",
    "    \"\"\"\n",
    "    Parametrize boxes using sigmoid to make them always valid and contained within the unit cube.\n",
    "\n",
    "    self.boxes[model, box, 2, dim] in Reals\n",
    "\n",
    "\n",
    "    In this parametrization, we first convert to the unit cube:\n",
    "\n",
    "    unit_cube_boxes = torch.sigmoid(self.boxes)  # shape: (model, box, 2, dim)\n",
    "\n",
    "    We now select the z/Z coordinates by taking the min/max over axis 2, i.e.\n",
    "\n",
    "    z, _ = torch.min(unit_cube_boxes, dim=2)\n",
    "    Z, _ = torch.max(unit_cube_boxes, dim=2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_models: int, num_boxes: int, dim: int, init_min_vol: float = default_init_min_vol,  **kwargs):\n",
    "        super().__init__()\n",
    "        unit_boxes = Boxes(num_models, num_boxes, dim, init_min_vol, **kwargs)\n",
    "        self._from_UnitBoxes(unit_boxes)\n",
    "        del unit_boxes\n",
    "\n",
    "\n",
    "    def _from_UnitBoxes(self, unit_boxes:Boxes):\n",
    "        boxes = unit_boxes().detach().clone()\n",
    "        self.boxes = Parameter(torch.log(boxes / (1-boxes)))\n",
    "\n",
    "\n",
    "    def forward(self, box_indices = slice(None, None, None), **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a Tensor representing the box embeddings specified by box_indices.\n",
    "\n",
    "        :param box_indices: A NamedTensor of the box indices\n",
    "        :param kwargs: Unused for now, but include this for future possible parameters.\n",
    "        :return: Tensor of shape (model, id, zZ, dim).\n",
    "        \"\"\"\n",
    "        unit_cube_boxes = torch.sigmoid(self.boxes)\n",
    "        z, _ = torch.min(unit_cube_boxes, dim=2)\n",
    "        Z, _ = torch.max(unit_cube_boxes, dim=2)\n",
    "        return torch.stack((z,Z), dim=2)\n",
    "    \n",
    "class AlignmentBoxes(Module):\n",
    "    \"\"\"\n",
    "    Parametrize boxes using sigmoid to make them always valid and contained within the unit cube.\n",
    "\n",
    "    self.boxes[model, box, 2, dim] in Reals\n",
    "\n",
    "\n",
    "    In this parametrization, we first convert to the unit cube:\n",
    "\n",
    "    unit_cube_boxes = torch.sigmoid(self.boxes)  # shape: (model, box, 2, dim)\n",
    "\n",
    "    We now select the z/Z coordinates by taking the min/max over axis 2, i.e.\n",
    "\n",
    "    z, _ = torch.min(unit_cube_boxes, dim=2)\n",
    "    Z, _ = torch.max(unit_cube_boxes, dim=2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_models: int, num_boxes: int, dim: int, init_min_vol: float = default_init_min_vol,  **kwargs):\n",
    "        super().__init__()\n",
    "        unit_boxes = Boxes(num_models, num_boxes, dim, init_min_vol, **kwargs)\n",
    "        self._from_UnitBoxes(unit_boxes)\n",
    "#         self.fc1 = nn.Linear(dim, 64)\n",
    "#         self.fc2 = nn.Linear(64, dim)\n",
    "        #self.fc = nn.Linear(dim, dim)\n",
    "        del unit_boxes\n",
    "\n",
    "\n",
    "    def _from_UnitBoxes(self, unit_boxes:Boxes):\n",
    "        boxes = unit_boxes().detach().clone()\n",
    "        self.boxes = Parameter(torch.log(boxes / (1-boxes)))\n",
    "\n",
    "\n",
    "    def forward(self, box_indices = slice(None, None, None), is_align=torch.tensor(0), **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a Tensor representing the box embeddings specified by box_indices.\n",
    "\n",
    "        :param box_indices: A NamedTensor of the box indices\n",
    "        :param kwargs: Unused for now, but include this for future possible parameters.\n",
    "        :return: Tensor of shape (model, id, zZ, dim).\n",
    "        \"\"\"\n",
    "        unit_cube_boxes = torch.sigmoid(self.boxes)\n",
    "        z, _ = torch.min(unit_cube_boxes, dim=2)\n",
    "        Z, _ = torch.max(unit_cube_boxes, dim=2)\n",
    "\n",
    "#         if is_align.tolist() == 1:\n",
    "# #             print(\"z before1:\", z[0])\n",
    "# #             print(\"w:\", self.fc.weight)\n",
    "# #             z = torch.nn.functional.relu(self.fc1(z))\n",
    "# #             z = self.fc2(z)\n",
    "# #             Z = torch.nn.functional.relu(self.fc1(Z))\n",
    "# #             Z = self.fc2(Z)\n",
    "#             z = self.fc1(z)\n",
    "#             Z = self.fc1(Z)\n",
    "#         else:\n",
    "#             z = self.fc2(z)\n",
    "#             Z = self.fc2(Z)\n",
    "\n",
    "#         if is_align.tolist() == 1:\n",
    "#             #print(\"before z\", z)\n",
    "#             #print(\"before Z\", Z)\n",
    "#             z = torch.nn.functional.relu(self.fc1(z))\n",
    "#             z = self.fc2(z)\n",
    "#             Z = torch.nn.functional.relu(self.fc1(Z))\n",
    "#             Z = self.fc2(Z)\n",
    "            #print(\"after z\", z)\n",
    "            #print(\"after Z\", Z)\n",
    "            \n",
    "#         if is_align.tolist() == 1:    \n",
    "#             z = self.fc(z)\n",
    "#             Z = self.fc(Z)\n",
    "\n",
    "        #print(\"z Z\", torch.stack((z,Z), dim=2))\n",
    "        return torch.stack((z,Z), dim=2)\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Downstream Model\n",
    "###############################################\n",
    "\n",
    "class WeightedSum(Module):\n",
    "    def __init__(self, num_models: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weights = Parameter(torch.rand(num_models))\n",
    "\n",
    "    def forward(self, box_vols: Tensor) -> Tensor:\n",
    "        return (F.softmax(self.weights, dim=0).unsqueeze(0) @ box_vols).squeeze()\n",
    "\n",
    "\n",
    "class LogWeightedSum(Module):\n",
    "    def __init__(self, num_models: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weights = Parameter(torch.rand(num_models))\n",
    "\n",
    "    def forward(self, log_box_vols: Tensor) -> Tensor:\n",
    "        return (torch.logsumexp(self.weights + log_box_vols, 0) - torch.logsumexp(self.weights, 0))\n",
    "\n",
    "\n",
    "class BoxModel(Module):\n",
    "    def __init__(self, BoxParamType: type, vol_func: Callable, evaluate_vol_func: Callable, intersection_func: Callable, join_func: Callable,\n",
    "                 align_intersection_func: Callable, num_models:int, num_boxes:int, dims:int,\n",
    "                 init_min_vol: float = default_init_min_vol, universe_box: Optional[Callable] = None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.box_embedding = BoxParamType(num_models, num_boxes, dims, init_min_vol, **kwargs)\n",
    "        self.vol_func = vol_func\n",
    "        self.evaluate_vol_func = evaluate_vol_func\n",
    "        self.intersection_func = intersection_func\n",
    "        self.align_intersection_func = align_intersection_func\n",
    "        self.join_func = join_func\n",
    "        self.fc1 = nn.Linear(dims*3, 128)\n",
    "        self.fc2 = nn.Linear(128, dims)\n",
    "        \n",
    "\n",
    "        if universe_box is None:\n",
    "            z = torch.zeros(dims)\n",
    "            Z = torch.ones(dims)\n",
    "            self.universe_box = lambda _: torch.stack((z,Z))[None, None]\n",
    "            self.universe_vol = lambda _: self.vol_func(self.universe_box(None)).squeeze()\n",
    "            self.clamp = True\n",
    "        else:\n",
    "            self.universe_box = universe_box\n",
    "            self.universe_vol = lambda b: self.vol_func(self.universe_box(b))\n",
    "            self.clamp = False\n",
    "\n",
    "        self.weights = WeightedSum(num_models)\n",
    "\n",
    "    def forward(self, box_indices: Tensor, is_align: Tensor = torch.tensor(0)) -> Dict:\n",
    "        #print(\"is_align in BoxModel:\", is_align)\n",
    "        # Unary\n",
    "        box_embeddings_orig = self.box_embedding(is_align = is_align)\n",
    "        #print(\"box_embeddings_orig\", box_embeddings_orig)\n",
    "        if self.clamp:\n",
    "            box_embeddings = box_embeddings_orig.clamp(0,1)\n",
    "        else:\n",
    "            box_embeddings = box_embeddings_orig\n",
    "        \n",
    "        box_embeddings = box_embeddings_orig\n",
    "        universe_vol = self.universe_vol(box_embeddings)\n",
    "\n",
    "#         print(\"attention here!!!\", self.vol_func(box_embeddings)[0][6], self.weights(self.vol_func(box_embeddings) / universe_vol)[6])\n",
    "        unary_probs = self.weights(self.vol_func(box_embeddings) / universe_vol)\n",
    "        #print(\"unary_probs:\", unary_probs)\n",
    "\n",
    "        \n",
    "        #print(\"after clamp box_embeddings\", box_embeddings)\n",
    "        # Conditional\n",
    "        A = box_embeddings[:, box_indices[:,0]]\n",
    "        B = box_embeddings[:, box_indices[:,1]]\n",
    "        \n",
    "#         print(box_indices[:,0])\n",
    "        \n",
    "#         print(box_embeddings.shape, A_self.shape, A_int_parents.shape, A_join_children.shape, A.shape)\n",
    "#         print(\"A_self\", A_self[0, 6])\n",
    "#         print(\"A_int_parents\", A_int_parents[0, 6])\n",
    "#         print(\"A_join_children\", A_join_children[0, 6])\n",
    "#         print(\"A\", A[0, 6])\n",
    "        \n",
    "              \n",
    "        #A_parents = box_embeddings[:, box_indices[:,0]]\n",
    "#         print(\"------------------------------------\")\n",
    "#         print(\"A\", A)\n",
    "#         print(\"B\", B)\n",
    "#         print(\"box_indices:\", box_indices, box_indices.shape)\n",
    "        \n",
    "        #print(\"A shape\", A.shape) #1 4 2 2\n",
    "        #print(\"A\", A[0, 0])\n",
    "        #print(\"B\", B[0, 0])\n",
    "        #print(\"intersection_func(A, B)\", self.intersection_func(A, B)[0, 0])\n",
    "        \n",
    "        \n",
    "        #A_int_B_vol = self.weights(self.vol_func(intersection(A, B)) / universe_vol) + torch.finfo(torch.float32).tiny\n",
    "        if is_align:\n",
    "            def make_box(fun, indexes, relation_dict):\n",
    "                relations = []\n",
    "                for index in indexes:\n",
    "                    relations_list = []\n",
    "                    for relation in relation_dict[index]:\n",
    "                        relations_list.append(box_embeddings[:, relation].tolist())\n",
    "                    relations.append(relations_list)\n",
    "\n",
    "                relations = torch.tensor(relations)\n",
    "                relations = torch.reshape(relations, (1, box_indices.shape[0], -1, 2, dims))\n",
    "                fun_relations = fun(relations)\n",
    "                return fun_relations\n",
    "\n",
    "            A_int_parents = make_box(self.align_intersection_func, box_indices[:,0].tolist(), parents_dict)\n",
    "            B_int_parents = make_box(self.align_intersection_func, box_indices[:,1].tolist(), parents_dict)\n",
    "            A_join_children = make_box(self.join_func, box_indices[:,0].tolist(), children_dict)\n",
    "            B_join_children = make_box(self.join_func, box_indices[:,1].tolist(), children_dict)\n",
    "\n",
    "\n",
    "            A = torch.cat((A, A_int_parents, A_join_children), -1)\n",
    "            B = torch.cat((B, B_int_parents, B_join_children), -1)\n",
    "                       \n",
    "            A = torch.nn.functional.relu(self.fc1(A))\n",
    "            A = self.fc2(A)\n",
    "            B = torch.nn.functional.relu(self.fc1(B))\n",
    "            B = self.fc2(B)\n",
    "            \n",
    "        \n",
    "#         A_int_B_vol = self.weights(self.vol_func(self.intersection_func(A, B)) / universe_vol) + torch.finfo(torch.float32).tiny\n",
    "#         B_vol = unary_probs[box_indices[:,1]] + torch.finfo(torch.float32).tiny\n",
    "        A_int_B_vol = self.vol_func(self.intersection_func(A, B)).squeeze(0) + torch.finfo(torch.float32).tiny\n",
    "        B_vol = self.vol_func(B).squeeze(0) + torch.finfo(torch.float32).tiny\n",
    "        P_A_given_B = torch.exp(A_int_B_vol - B_vol)\n",
    "        \n",
    "        \n",
    "        A_int_B_vol_evaluate = self.evaluate_vol_func(self.intersection_func(A, B)).squeeze(0)\n",
    "        B_vol_evaluate = self.evaluate_vol_func(B).squeeze(0)\n",
    "        P_A_given_B_evaluate = torch.exp(A_int_B_vol_evaluate - B_vol_evaluate)\n",
    "        \n",
    "            \n",
    "        \n",
    "#         A_int_B_vol = self.weights(self.vol_func(self.intersection_func(A, B)) / universe_vol) + torch.finfo(torch.float32).tiny\n",
    "        \n",
    "\n",
    "# #         print(\"A\", A)\n",
    "# #         print(\"B\", B)\n",
    "#         #print(\"box_indices\", box_indices)\n",
    "#         print(\"A_int_B_vol:\", A_int_B_vol)\n",
    "        \n",
    "#         B_vol = unary_probs[box_indices[:,1]] + torch.finfo(torch.float32).tiny\n",
    "#         print(\"B_vol:\", B_vol)\n",
    "        \n",
    "#         P_A_given_B = torch.exp(A_int_B_vol - B_vol)\n",
    "# #         P_A_given_B = torch.exp(torch.log(A_int_B_vol) - torch.log(B_vol))\n",
    "#         print(\"P_A_given_B:\", P_A_given_B)\n",
    "    \n",
    "#         A_int_B = self.intersection_func(A, B)\n",
    "#         print(\"A_int_B\", A_int_B)\n",
    "#         print(\"A_int_B\", A_int_B[:,:,1] - A_int_B[:,:,0], F.softplus(A_int_B[:,:,1] - A_int_B[:,:,0]))\n",
    "#         print(torch.prod(F.softplus(A_int_B[:,:,1] - A_int_B[:,:,0]), dim=-1))\n",
    "# #         B_box = box_embeddings[:, box_indices[:,1]]\n",
    "#         print(\"A\", A)\n",
    "#         print(\"B\", B)\n",
    "#         print(\"B\", B[:,:,1] - B[:,:,0], F.softplus(B[:,:,1] - B[:,:,0]))\n",
    "#         print(torch.prod(F.softplus(B[:,:,1] - B[:,:,0]), dim=-1))\n",
    "    \n",
    "#         torch.prod(F.softplus(boxes[:,:,1] - boxes[:,:,0]), dim=-1)\n",
    "#         print(\"------------------------------------\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        # symmetric same\n",
    "        # print(\"you are in right place!\")\n",
    "#         A = box_embeddings[:, box_indices[:,0]]\n",
    "#         B = box_embeddings[:, box_indices[:,1]]\n",
    "#         A_int_B_vol = self.weights(self.vol_func(intersection(A, B)) / universe_vol) + torch.finfo(torch.float32).tiny\n",
    "#         A_join_B_vol = self.weights(self.vol_func(join(A, B)) / universe_vol) + torch.finfo(torch.float32).tiny\n",
    "#         P_A_given_B = torch.exp(torch.log(A_int_B_vol) - torch.log(A_join_B_vol))\n",
    "#         try:\n",
    "#             print(\"box_embeddings_orig:\", box_embeddings_orig)\n",
    "#             print(\"box_indices:\", box_indices[0])\n",
    "#             print(\"A:\", A[0][0])\n",
    "#             print(\"B:\", B[0][0])\n",
    "#             print(\"intersection(A, B):\", intersection(A, B)[0][0])\n",
    "#             print(\"self.vol_func(intersection(A, B)):\", self.vol_func(intersection(A, B))[0][0])\n",
    "#             print(\"universe_vol:\", universe_vol)\n",
    "#             print(\"self.weights(self.vol_func(intersection(A, B)) / universe_vol):\", self.weights(self.vol_func(intersection(A, B)) / universe_vol)[0])\n",
    "#             print(\"A_int_B_vol:\", A_int_B_vol[0])\n",
    "#             print(\"unary_probs:\", unary_probs)\n",
    "#             print(\"B_vol:\", B_vol[0])\n",
    "#             print(\"P_A_given_B:\", P_A_given_B[0])\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "        if 1:\n",
    "            def make_box(fun, indexes, relation_dict):\n",
    "                relations = []\n",
    "                for index in indexes:\n",
    "                    relations_list = []\n",
    "                    for relation in relation_dict[index]:\n",
    "                        relations_list.append(box_embeddings[:, relation].tolist())\n",
    "                    relations.append(relations_list)\n",
    "\n",
    "                relations = torch.tensor(relations)\n",
    "                relations = torch.reshape(relations, (1, 20, -1, 2, dims))\n",
    "                fun_relations = fun(relations)\n",
    "                return fun_relations\n",
    "\n",
    "            box_embeddings_int_parents = make_box(self.align_intersection_func, np.arange(0, 20), parents_dict)\n",
    "            box_embeddings_join_children = make_box(self.join_func, np.arange(0, 20), children_dict)\n",
    "\n",
    "#             box_embeddings = torch.cat((box_embeddings, torch.randn_like(box_embeddings), torch.randn_like(box_embeddings)), -1)\n",
    "            \n",
    "            #print(box_embeddings.shape, box_embeddings_int_parents.shape, box_embeddings_join_children.shape)\n",
    "            box_embeddings = torch.cat((box_embeddings, box_embeddings_int_parents, box_embeddings_join_children), -1)\n",
    "                       \n",
    "            box_embeddings = torch.nn.functional.relu(self.fc1(box_embeddings))\n",
    "            box_embeddings = self.fc2(box_embeddings)\n",
    "           \n",
    "            \n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"unary_probs\": unary_probs,\n",
    "            \"box_embeddings_orig\": box_embeddings_orig,\n",
    "            \"box_embeddings\": box_embeddings,\n",
    "            \"A\": A,\n",
    "            \"B\": B,\n",
    "            \"P(A|B)\": P_A_given_B,\n",
    "            \"P(A|B)_evaluate\": P_A_given_B_evaluate,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(idx, color, beo, dim=0):\n",
    "    node = beo[idx,:,dim:dim+2].cpu().detach()\n",
    "    \n",
    "    width  = abs(node[1,0] - node[0,0])\n",
    "    height = abs(node[1,1] - node[0,1])\n",
    "    \n",
    "    node_min = torch.min(node, dim=0).values\n",
    "    \n",
    "    rect = patches.Rectangle(node_min, width, height, linewidth=2, edgecolor=color, fill=True, facecolor=color, alpha=0.5)\n",
    "    \n",
    "    return rect, node_min[0].item(), node_min[1].item(), idx.item(), width, height\n",
    "\n",
    "def draw(beo, box_idx):\n",
    "    fig,ax = plt.subplots(1, figsize=(8,8), facecolor=\"white\")\n",
    "\n",
    "    #box_idx = torch.tensor(np.arange(2, 7))\n",
    "\n",
    "#     colors = list(mcolors.CSS4_COLORS)\n",
    "    colors = ['xkcd:orange red', 'xkcd:sunflower', 'xkcd:magenta', 'xkcd:baby purple',\n",
    "              'xkcd:chartreuse',  'xkcd:greenish', 'xkcd:mint', 'xkcd:sky blue',\n",
    "              'xkcd:slate blue', 'xkcd:deep red', 'xkcd:greyish brown', 'xkcd:canary yellow',\n",
    "              'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque']\n",
    "\n",
    "    rectangles = []\n",
    "    labels = []\n",
    "\n",
    "    # rect1 = patches.Rectangle(one_node_min, width, height,linewidth=2,edgecolor='r', fill=True, facecolor='r', alpha=0.3)\n",
    "    # rect2 = patches.Rectangle((0.5,0.1),.1,.3,linewidth=2,edgecolor='b', fill=True, facecolor='b', alpha=0.3)\n",
    "    left = 100\n",
    "    right = -100\n",
    "    top = -100\n",
    "    bottom = 100\n",
    "    # 0, 4\n",
    "    for i, bidx in enumerate(box_idx):\n",
    "        rect, xmin, ymin, name, w, h = get_boxes(bidx, colors[i], beo, dim=0)\n",
    "        print(bidx, xmin, ymin, xmin + w, ymin + h)\n",
    "#         print(bidx, beo[bidx].cpu().detach())\n",
    "        if xmin < left:\n",
    "            left = xmin\n",
    "        if ymin < bottom:\n",
    "            bottom = ymin\n",
    "        if xmin + w > right:\n",
    "            right = xmin+w\n",
    "        if ymin + h > top:\n",
    "            top = ymin+h\n",
    "\n",
    "        ax.text(xmin, ymin, bidx.item(), fontsize=14)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        if bidx.item() < split_num:\n",
    "            labels.append(str(name) + \" (mouse)\")\n",
    "        else:\n",
    "            labels.append(str(name) + \" (human)\")\n",
    "\n",
    "    # ax.add_patch(rect2)\n",
    "\n",
    "    # ax.set_xlim([-.1, 1.5])\n",
    "    # ax.set_ylim([-.1, 1.5])\n",
    "\n",
    "    \n",
    "#     adjust_left = lambda x, y: x - (y-x)*0.1\n",
    "#     adjust_right = lambda x, y: x + (y-x)*0.1\n",
    "#     ax.set_xlim([adjust_left(left, right), adjust_right(left, right)])\n",
    "#     ax.set_ylim([adjust_left(bottom, top), adjust_right(bottom, top)])\n",
    "    \n",
    "    ax.set_xlim([left, right])\n",
    "    ax.set_ylim([bottom, top])\n",
    "\n",
    "    # ax.legend(labels, fontsize=18, loc=10, )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 20\n",
    "# lr = 0.07342406890949607\n",
    "lr = 5e-2\n",
    "#lr = 0.2\n",
    "rns_ratio = 5\n",
    "#box_type = MinMaxSigmoidBoxes, AlignmentBoxes\n",
    "use_unary = False\n",
    "unary_weight = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_model = BoxModel(\n",
    "    BoxParamType=AlignmentBoxes,\n",
    "    vol_func=log_soft_volume,\n",
    "    evaluate_vol_func=log_clamp_volume,\n",
    "    #vol_func=log_soft_volume_adjusted,\n",
    "    intersection_func = intersection,\n",
    "    align_intersection_func = multi_intersection,\n",
    "    join_func = multi_join,\n",
    "    #intersection_func = intersection,\n",
    "    #intersection_func = gumbel_intersection,\n",
    "    \n",
    "    \n",
    "    num_models=1,\n",
    "    num_boxes=num_boxes,\n",
    "    dims=dims,\n",
    "    method=\"tree\").to(device)\n",
    "\n",
    "\n",
    "#### IF YOU ARE LOADING FROM JULIA WITH ratio_neg=0, train_dl WILL ONLY CONTAIN POSITIVE EXAMPLES\n",
    "#### THIS MEANS YOUR MODEL SHOULD USE NEGATIVE SAMPLING DURING TRAINING\n",
    "train_dl = TensorDataLoader(train, batch_size=52, shuffle=True)\n",
    "\n",
    "mouse_dl = TensorDataLoader(mouse_eval, batch_size=52)\n",
    "human_dl = TensorDataLoader(human_eval, batch_size=52)\n",
    "\n",
    "eval_dl = [mouse_dl, human_dl]\n",
    "\n",
    "opt = torch.optim.Adam(box_model.parameters(), lr=lr, weight_decay=0)\n",
    "#opt = torch.optim.AdamW(box_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def kl_div_sym(p: Tensor, q: Tensor, eps: float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    #print(\"p\", p)\n",
    "    #print(\"q\", q)\n",
    "    #print(\"kl_div_term(p, q, eps)\", kl_div_term(p, q, eps))\n",
    "    #print(\"kl_div_term(1-p, 1-q, eps)\", kl_div_term(1-p, 1-q, eps))\n",
    "    return kl_div_term(p, q, eps) + kl_div_term(1-p, 1-q, eps)\n",
    "\n",
    "def kl_div_term(p: Tensor, q: Tensor, eps: float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    return F.kl_div(torch.log(p.clamp_min(eps)), q.clamp_min(eps), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cond_kl_loss(model_out: ModelOutput, target: Tensor, eps: float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    return kl_div_sym(model_out[\"P(A|B)\"], target, eps).mean()\n",
    "\n",
    "def human_cond_kl_loss(model_out: ModelOutput, target: Tensor, eps: float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    return kl_div_sym(model_out[\"P(A|B)\"], target, eps).mean()\n",
    "\n",
    "def mouse_cond_kl_loss(model_out: ModelOutput, target: Tensor, eps: float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    return kl_div_sym(model_out[\"P(A|B)\"], target, eps).mean()\n",
    "\n",
    "def align_cond_kl_loss(model_out: ModelOutput, target: Tensor, eps: float = torch.finfo(torch.float32).tiny) -> Tensor:\n",
    "    #print(\"ModelOutput\", ModelOutput)\n",
    "    #print(\"target\", target)\n",
    "    #print(\"align_cond_kl_loss\", kl_div_sym(model_out[\"P(A|B)\"], target, eps).mean())\n",
    "    return kl_div_sym(model_out[\"P(A|B)\"], target, eps).mean()\n",
    "\n",
    "# See boxes/loss_functions.py file for more options. Note that you may have to changed them to fit your use case.\n",
    "# Also note that \"kl_div_sym\" is just binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "global my_batch_in\n",
    "global my_batch_out\n",
    "my_batch_in = None\n",
    "my_batch_out = None\n",
    "\n",
    "@dataclass\n",
    "class Learner:\n",
    "    train_dl: DataLoader\n",
    "    model: Module\n",
    "    loss_fn: Callable\n",
    "    opt: optim.Optimizer\n",
    "    callbacks: CallbackCollection = field(default_factory=CallbackCollection)\n",
    "    recorder: Recorder = field(default_factory=Recorder)\n",
    "    categories: bool = False\n",
    "    use_wandb: bool = False\n",
    "    reraise_keyboard_interrupt: bool = False\n",
    "    reraise_stop_training_exceptions: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.progress = Progress(0,0,len(self.train_dl))\n",
    "        self.callbacks.learner_post_init(self)\n",
    "\n",
    "    #the split parameter will be used to find human/mouse/align data, so you need change it when using diff dataset(index)\n",
    "    def split_data(self, batch_in, batch_out, split):\n",
    "        category = torch.zeros(size=(batch_in.shape[0],), dtype=int)\n",
    "\n",
    "        batch_class = batch_in > split\n",
    "\n",
    "        for i, (a,b) in enumerate(batch_class):\n",
    "            if not a and not b:\n",
    "                category[i] = 0\n",
    "            elif a and b:\n",
    "                category[i] = 1\n",
    "            else:\n",
    "                category[i] = 2\n",
    "\n",
    "        self.mouse_in = batch_in[category == 0]\n",
    "        self.human_in = batch_in[category == 1]\n",
    "        self.align_in = batch_in[category == 2]\n",
    "\n",
    "        self.mouse_out = batch_out[category == 0]\n",
    "        self.human_out = batch_out[category == 1]\n",
    "        self.align_out = batch_out[category == 2]\n",
    "\n",
    "        # INPUT TO THE MODEL:\n",
    "        data_in = (self.mouse_in, self.human_in, self.align_in)\n",
    "        # TARGET/LABEL:\n",
    "        data_out = (self.mouse_out, self.human_out, self.align_out)\n",
    "\n",
    "        return data_in, data_out\n",
    "\n",
    "    def TensorNaN(self, size:Union[None,List[int], Tuple[int]]=None, device=None, requires_grad:bool=True):\n",
    "        if size is None:    \n",
    "            return torch.tensor(float('nan'), device=device, requires_grad=requires_grad)\n",
    "        else:\n",
    "            return float('nan') * torch.zeros(size=size, device=device, requires_grad=requires_grad)\n",
    "\n",
    "\n",
    "    def train(self, epochs, progress_bar = True):\n",
    "        global my_batch_in\n",
    "        global my_batch_out\n",
    "        try:\n",
    "            self.callbacks.train_begin(self)\n",
    "            for epoch in trange(epochs, desc=\"Overall Training:\", disable=not progress_bar):\n",
    "                self.callbacks.epoch_begin(self)\n",
    "                \n",
    "                if epoch%10==0:\n",
    "                    beo = box_model(torch.tensor([[1, 1]]), torch.tensor(1))['box_embeddings'].squeeze(0)\n",
    "                    print(\"pos alignment data:\")\n",
    "                    box_idx = torch.tensor([2, 11, 4, 13, 9, 17, 6, 15])\n",
    "                    #box_idx = torch.tensor([2, 4, 6, 9, 11, 13, 15, 17, 5, 16, 3, 14])\n",
    "                    draw(beo, box_idx)\n",
    "\n",
    "                    print(\"neg alignment data:\")\n",
    "                    box_idx = torch.tensor([4, 14, 6, 16, 5, 15, 11])\n",
    "                    draw(beo, box_idx)\n",
    "                    \n",
    "                \n",
    "                for iteration, batch in enumerate(tqdm(self.train_dl, desc=\"Current Batch:\", leave=False, disable=not progress_bar)):\n",
    "                    # draw the boxes at every beginning of epoch\n",
    "#                     print(\"tree data mouse:\")\n",
    "#                     beo = box_model(torch.tensor([[1, 1]]), torch.tensor(0))['box_embeddings_orig'].squeeze(0)\n",
    "#                     box_idx = torch.tensor(np.arange(1, 10))\n",
    "# #                     box_idx = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "# #                     box_idx = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "#                     draw(beo, box_idx)\n",
    "    \n",
    "#                     print(\"tree data human:\")\n",
    "#                     beo = box_model(torch.tensor([[1, 1]]), torch.tensor(0))['box_embeddings_orig'].squeeze(0)\n",
    "#                     box_idx = torch.tensor(np.arange(10, 20))\n",
    "# #                     box_idx = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "# #                     box_idx = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "#                     draw(beo, box_idx)\n",
    "                    \n",
    "    \n",
    "#                     print(\"alignment data:\")\n",
    "#                     beo = box_model(torch.tensor([[1, 1]]), torch.tensor(1))['box_embeddings'].squeeze(0)\n",
    "#                     box_idx = torch.tensor([4, 6, 13, 15, 5, 16, 3, 14])\n",
    "#                     draw(beo, box_idx)\n",
    "                    \n",
    "#                     print(\"alignment data:\")\n",
    "#                     beo = box_model(torch.tensor([[1, 1]]), torch.tensor(1))['box_embeddings'].squeeze(0)\n",
    "#                     box_idx = torch.tensor(np.arange(1, 20))\n",
    "# ##                     box_idx = torch.tensor([2, 11, 4, 13, 6, 15, 9, 17])\n",
    "# #                     box_idx = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "#                     draw(beo, box_idx)\n",
    "    \n",
    "\n",
    "#                     print(\"batch\", batch)\n",
    "                    if len(batch) == 2: # KLUDGE\n",
    "                        self.batch_in, self.batch_out = batch\n",
    "                    else:\n",
    "                        self.batch_in = batch[0]\n",
    "                        self.batch_out = None\n",
    "                    self.progress.increment()\n",
    "                    self.callbacks.batch_begin(self)\n",
    "                    self.opt.zero_grad()\n",
    "                    \n",
    "                    if self.categories:\n",
    "                        self.data_in, self.data_out = self.split_data(self.batch_in, self.batch_out, split=split_num)\n",
    "                        #9 is max mouse index  \n",
    "                        self.model_pred = []\n",
    "                        count = 0\n",
    "                        for item in self.data_in:\n",
    "                            if len(item)>0:\n",
    "                                if count<2:\n",
    "                                    self.model_pred.append(self.model(item, is_align=torch.tensor(0)))\n",
    "                                else:\n",
    "                                    self.model_pred.append(self.model(item, is_align=torch.tensor(1)))\n",
    "                            else:\n",
    "                                self.model_pred.append({'P(A|B)':self.TensorNaN(device=self.batch_in.device)})\n",
    "                            count+=1\n",
    "                        #self.model_pred = [self.model(item, is_align=is_align) if len(item)>0 else {'P(A|B)':self.TensorNaN(device=self.batch_in.device)} for item in self.data_in]\n",
    "                        #print(\"self.model_pred:\", self.model_pred)\n",
    "#                         print(\"self.data_in:\", self.data_in)\n",
    "#                         print(\"self.data_out:\", self.data_out)\n",
    "                        self.loss = self.loss_fn(self.model_pred, self.data_out, self, self.recorder, categories=True, use_wandb=self.use_wandb)   \n",
    "#                         print(\"previous loss:\", self.loss)\n",
    "                    else:\n",
    "                        self.model_out = self.model(self.batch_in, is_align=False)\n",
    "                        if self.batch_out is None:\n",
    "                            self.loss = self.loss_fn(self.model_out, self, self.recorder, categories=True, use_wandb=self.use_wandb)\n",
    "                        else:\n",
    "                            self.loss = self.loss_fn(self.model_out, self.batch_out, self, self.recorder, categories=True, use_wandb=self.use_wandb)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Log metrics inside your training loop\n",
    "                    if self.use_wandb:\n",
    "                        metrics = {'epoch': epoch, 'loss': self.loss}\n",
    "                        wandb.log(metrics)\n",
    "\n",
    "                    # print(self.recorder.dataframe)\n",
    "                    #print(\"loss:\", self.loss)\n",
    "                    self.loss.backward()\n",
    "                    self.callbacks.backward_end(self)\n",
    "                    self.opt.step()\n",
    "                    self.callbacks.batch_end(self)\n",
    "                # print(self.recorder.dataframe)\n",
    "                \n",
    "                # run evaluating at the end of every epoch\n",
    "                #self.evaluation(np.arange(0.1, 1, 0.1))\n",
    "                self.evaluation([Threshold])\n",
    "                \n",
    "                \n",
    "                self.callbacks.epoch_end(self)\n",
    "        except StopTrainingException as e:\n",
    "            print(e)\n",
    "            if self.reraise_stop_training_exceptions:\n",
    "                raise e\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"Stopped training at {self.progress.partial_epoch_progress()} epochs due to keyboard interrupt.\")\n",
    "            if self.reraise_keyboard_interrupt:\n",
    "                raise KeyboardInterrupt\n",
    "        finally:\n",
    "            self.callbacks.train_end(self)\n",
    "\n",
    "\n",
    "    def evaluation(self, trials, progress_bar=True):\n",
    "        with torch.no_grad():\n",
    "            # self.callbacks.eval_begin(self)\n",
    "            for t in trials:\n",
    "                self.callbacks.eval_align(self, t)\n",
    "            self.callbacks.metric_plots(self)\n",
    "            self.callbacks.bias_metric(self)\n",
    "            self.callbacks.eval_end(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from typing import *\n",
    "#from .learner import Learner, Recorder\n",
    "from collections import OrderedDict\n",
    "#import wandb\n",
    "\n",
    "FuncInput = Union[OrderedDict, dict, Collection[Union[Callable, Tuple[float, Callable]]]]\n",
    "\n",
    "def func_list_to_dict(*func_list: FuncInput) -> Dict[str, Callable]:\n",
    "    \"\"\"\n",
    "\n",
    "    :param func_list: List of functions or tuples (weight, function)\n",
    "    :type func_list: Union[Collection[Union[Callable, Tuple[float, Callable]]], Dict[str, Callable]]\n",
    "\n",
    "    :return: Ordered Dictionary of {name: function}\n",
    "    :rtype: OrderedDict[str, Callable]\n",
    "    \"\"\"\n",
    "    if type(func_list) == OrderedDict or type(func_list) == dict:\n",
    "        return func_list\n",
    "    func_dict = OrderedDict()\n",
    "    for f in func_list:\n",
    "        if type(f) is tuple:\n",
    "            func_dict[f\"{f[0]}*{f[1].__name__}\"] = lambda *args, weight=f[0], func=f[1], **kwargs: weight * func(*args, **kwargs)\n",
    "        else:\n",
    "            func_dict[f.__name__] = f\n",
    "    return func_dict\n",
    "\n",
    "def unweighted_func_dict(*func_list: FuncInput) -> Dict[str, Callable]:\n",
    "    if type(func_list) == OrderedDict or type(func_list) == dict:\n",
    "        return func_list\n",
    "    func_dict = OrderedDict()\n",
    "    for f in func_list:\n",
    "        if type(f) is tuple:\n",
    "            f = f[1]\n",
    "        func_dict[f.__name__] = f\n",
    "    return func_dict\n",
    "\n",
    "def isnan(x):\n",
    "    return (x != x)\n",
    "\n",
    "class LossPieces(Module):\n",
    "\n",
    "    def __init__(self, *loss_funcs: FuncInput):\n",
    "        \"\"\"\n",
    "\n",
    "        :param functions: List of functions or tuples (weight, function)\n",
    "        :type functions: Collection[Union[Callable, Tuple[float, Callable]]]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.unweighted_funcs = unweighted_func_dict(*loss_funcs)\n",
    "        self.loss_funcs = func_list_to_dict(*loss_funcs)\n",
    "\n",
    "    def forward(self, model_out, true_out: Tensor,\n",
    "                learner: Optional[Learner] = None,\n",
    "                recorder: Optional[Recorder] = None,\n",
    "                weighted: bool = True,\n",
    "                categories: bool = False,\n",
    "                use_wandb: bool = False,\n",
    "                **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Weighted sum of all loss functions. Tracks values in Recorder.\n",
    "        \"\"\"\n",
    "        if weighted:\n",
    "            loss_funcs = self.loss_funcs\n",
    "        else:\n",
    "            loss_funcs = self.unweighted_funcs\n",
    "        \n",
    "        grad_status = torch.is_grad_enabled()\n",
    "        if learner is None:\n",
    "            torch.set_grad_enabled(False)\n",
    "        \n",
    "        # change the loss function for margin loss\n",
    "        try:\n",
    "            if categories:\n",
    "                loss_pieces = {}\n",
    "                for k,l in loss_funcs.items():\n",
    "                    if 'mouse' in k:\n",
    "                        loss_pieces['mouse_cond_kl_loss'] = l(model_out[0], true_out[0])\n",
    "                    elif 'human' in k:\n",
    "                        loss_pieces['human_cond_kl_loss'] = l(model_out[1], true_out[1])\n",
    "                    else:\n",
    "                        loss_pieces['align_cond_kl_loss'] = weight_alignment_loss*l(model_out[2], true_out[2])\n",
    "#                     if 'mouse' in k:\n",
    "#                         loss_pieces[k] = l(model_out[0], true_out[0])\n",
    "#                     elif 'human' in k:\n",
    "#                         loss_pieces[k] = l(model_out[1], true_out[1])\n",
    "#                     else:\n",
    "#                         loss_pieces[k] = l(model_out[2], true_out[2])\n",
    "                if use_wandb:\n",
    "                    # Log metrics inside your training loop\n",
    "                    metrics = {'mouse_loss': torch.sum(loss_pieces['mouse_cond_kl_loss']), \n",
    "                               'human_loss': torch.sum(loss_pieces['human_cond_kl_loss']), \n",
    "                               'alignmnt_loss': torch.sum(loss_pieces['align_cond_kl_loss'])}\n",
    "                    wandb.log(metrics)\n",
    "                \n",
    "                loss = 0\n",
    "                for key,val in loss_pieces.items():\n",
    "                    if not isnan(val):\n",
    "                        loss += val\n",
    "                        print(\"key\", key)\n",
    "                        print(\"val\", val)\n",
    "                        \n",
    "\n",
    "            else:\n",
    "                loss_pieces = {k: l(model_out, true_out) for k, l in loss_funcs.items()}\n",
    "                loss = sum(loss_pieces.values())\n",
    "          \n",
    "\n",
    "            loss_pieces['loss'] = loss\n",
    "            if learner is not None:\n",
    "                if recorder is not None:\n",
    "                    recorder.update_(loss_pieces, learner.progress.partial_epoch_progress())\n",
    "                else:\n",
    "                    self.recorder.update_(loss_pieces, learner.progress.partial_epoch_progress())\n",
    "        finally:\n",
    "            torch.set_grad_enabled(grad_status)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this dataset we had unary probabilities as well as conditional probabilities. Our loss function will be a sum of these, which is provided by the following loss function wrapper:\n",
    "\n",
    "# if use_unary:\n",
    "#     loss_func = LossPieces(mean_cond_kl_loss, (unary_weight, mean_unary_kl_loss(unary_prob)))\n",
    "# else:\n",
    "#     loss_func = LossPieces(mean_cond_kl_loss)\n",
    "\n",
    "loss_func = LossPieces((mouse_cond_kl_loss), (human_cond_kl_loss), (5e-2, align_cond_kl_loss))\n",
    "\n",
    "metrics = [metric_hard_accuracy, metric_hard_f1]\n",
    "align_metrics = [metric_hard_accuracy_align, metric_hard_f1_align, metric_hard_accuracy_align_mean, metric_hard_f1_align_mean]\n",
    "\n",
    "rec_col = RecorderCollection()\n",
    "\n",
    "# #threshold = np.arange(0.1, 1, 0.1)\n",
    "# threshold = [0.5]\n",
    "\n",
    "callbacks = CallbackCollection(\n",
    "#     LossCallback(rec_col.train, train),\n",
    "#     LossCallback(rec_col.dev, dev),\n",
    "    \n",
    "    *(MetricCallback(rec_col.onto, human_eval, \"human\", m) for m in metrics),\n",
    "    *(MetricCallback(rec_col.onto, mouse_eval, \"mouse\", m) for m in metrics),\n",
    "    *(EvalAlignment(rec_col.train_align, tr_align, \"train_align\", m) for m in align_metrics),\n",
    "    *(EvalAlignment(rec_col.dev_align, dev, \"dev_align\", m) for m in align_metrics),\n",
    "    \n",
    "    #JustGiveMeTheData(rec_col.probs, dev, get_probabilities),\n",
    "    #BiasMetric(rec_col.bias, dev, pct_of_align_cond_on_human_as_min),\n",
    "    #PlotMetrics(rec_col.dev_roc_plot, dev, roc_plot),\n",
    "    #PlotMetrics(rec_col.dev_pr_plot, dev, pr_plot),\n",
    "    #PlotMetrics(rec_col.tr_roc_plot, tr_align, roc_plot),\n",
    "    #PlotMetrics(rec_col.tr_pr_plot, tr_align, pr_plot),\n",
    "    #MetricCallback(rec_col.train, train, 'train', metric_pearson_r),\n",
    "    #MetricCallback(rec_col.train, train, 'train', metric_spearman_r),\n",
    "    #MetricCallback(rec_col.dev, dev, 'dev', metric_pearson_r),\n",
    "    #MetricCallback(rec_col.dev, dev, 'dev', metric_spearman_r),\n",
    "#     MetricCallback(rec_col.train, train, 'train', mean_reciprocal_rank),\n",
    "#     MetricCallback(rec_col.dev, dev, 'dev', mean_reciprocal_rank),\n",
    "#     PercentIncreaseEarlyStopping(rec_col.dev, \"mean_cond_kl_loss\", 0.25, 10),\n",
    "#     PercentIncreaseEarlyStopping(rec_col.dev, \"mean_cond_kl_loss\", 0.5),\n",
    "#     PercentIncreaseEarlyStopping(rec_col.dev, \"mouse_cond_kl_loss\", 0.25, 10),\n",
    "#     PercentIncreaseEarlyStopping(rec_col.dev, \"mouse_cond_kl_loss\", 0.5),\n",
    "    GradientClipping(-1000,1000),\n",
    "#     RandomNegativeSampling(num_boxes, rns_ratio),\n",
    "    StopIfNaN(),\n",
    ")\n",
    "\n",
    "# l = Learner(train_dl, box_model, loss_func, opt, callbacks, recorder = rec_col.learn)\n",
    "l = Learner(train_dl, box_model, loss_func, opt, callbacks, recorder = rec_col.learn, categories=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd8ef4401f54e438cc667618cf0c6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Overall Training:'), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos alignment data:\n",
      "tensor(2) -0.08483614027500153 0.37479981780052185 tensor(0.0564380437135696) tensor(0.5107228159904480)\n",
      "tensor(11) -0.044454626739025116 0.3028077185153961 tensor(0.0951082035899162) tensor(0.4884748458862305)\n",
      "tensor(4) 0.02584262192249298 0.1820831596851349 tensor(0.1686311066150665) tensor(0.5893871188163757)\n",
      "tensor(13) 0.05804925411939621 0.35769981145858765 tensor(0.1259796619415283) tensor(0.6062905788421631)\n",
      "tensor(9) 0.16987916827201843 0.3488696813583374 tensor(0.2574813365936279) tensor(0.4530902802944183)\n",
      "tensor(17) 0.09059230238199234 0.3696792423725128 tensor(0.1201785281300545) tensor(0.5191607475280762)\n",
      "tensor(6) 0.07726072520017624 0.3719407320022583 tensor(0.0880022421479225) tensor(0.5043252110481262)\n",
      "tensor(15) 0.03400997817516327 0.3452932834625244 tensor(0.0610499456524849) tensor(0.5296712517738342)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg alignment data:\n",
      "tensor(4) 0.02584262192249298 0.1820831596851349 tensor(0.1686311066150665) tensor(0.5893871188163757)\n",
      "tensor(14) 0.05204976350069046 0.292916476726532 tensor(0.1373203396797180) tensor(0.5618486404418945)\n",
      "tensor(6) 0.07726072520017624 0.3719407320022583 tensor(0.0880022421479225) tensor(0.5043252110481262)\n",
      "tensor(16) 0.11007224768400192 0.40974968671798706 tensor(0.1505877077579498) tensor(0.5162158608436584)\n",
      "tensor(5) 0.11304252594709396 0.3871040344238281 tensor(0.1142951771616936) tensor(0.6324584484100342)\n",
      "tensor(15) 0.03400997817516327 0.3452932834625244 tensor(0.0610499456524849) tensor(0.5296712517738342)\n",
      "tensor(11) -0.044454626739025116 0.3028077185153961 tensor(0.0951082035899162) tensor(0.4884748458862305)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(3.8889667987823486, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(3.9282388687133789, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0341669768095016, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0., 0., 0.])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0., 0., 0.])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([0., 0., 0., 0., 0.])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([5.3931493736776370e-40, 4.2038953929744512e-45, 0.0000000000000000e+00,\n",
      "        2.6905771294115083e-40, 0.0000000000000000e+00])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(3.7201948165893555, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(3.7618417739868164, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0802892372012138, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.0000000000000000e+00, 1.6341052949428558e-02, 2.5223372357846707e-44])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([9.4039449468255043e-03, 2.2442845880010189e-39, 0.0000000000000000e+00])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([2.6602782540158771e-39, 3.7889762426220373e-38, 0.0000000000000000e+00,\n",
      "        2.5255431264113530e-38, 0.0000000000000000e+00])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([3.2260813757353871e-38, 4.0506353799312460e-40, 2.8954707869696900e-39,\n",
      "        2.7707210319622518e-38, 0.0000000000000000e+00])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(3.5522210597991943, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(3.6023862361907959, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0841033831238747, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.0000000000000000e+00, 0.0000000000000000e+00, 3.7170702934833665e-40])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0.0000000000000000e+00, 8.8342059086429443e-41, 0.0000000000000000e+00])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([0.0000000000000000e+00, 0.0000000000000000e+00, 1.9478048654114957e-43,\n",
      "        7.9232427964623456e-39, 0.0000000000000000e+00])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([0.0000000000000000e+00, 0.0000000000000000e+00, 0.0000000000000000e+00,\n",
      "        7.4466401692685104e-41, 0.0000000000000000e+00])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(3.3892474174499512, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(3.4502000808715820, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0463868863880634, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([9.4645187258720398e-02, 4.1599228978157043e-02, 8.6301952251233160e-05])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0.0000000000000000e+00, 7.8631257638335228e-03, 1.9728530754613018e-39])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([1.9937187433242798e-02, 2.0886752980823731e-38, 3.4807638371603389e-07,\n",
      "        1.9903903257423269e-40, 3.9688259363174438e-02])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([3.6484360694885254e-02, 0.0000000000000000e+00, 3.0981122981756926e-03,\n",
      "        4.7181719293816591e-42, 0.0000000000000000e+00])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(3.2326579093933105, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(3.3046212196350098, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0352905541658401, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.2376116663217545, 0.1923360973596573, 0.0006965094362386])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0.0067644217051566, 0.0845370143651962, 0.0290204118937254])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([1.0769021511077881e-01, 7.2045505046844482e-01, 5.9650706134561915e-06,\n",
      "        1.1279973387718201e-01, 2.2784239053726196e-01])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([1.6792531311511993e-01, 3.2475057309966360e-07, 8.2855962216854095e-02,\n",
      "        1.2571537867188454e-02, 4.4320024549961090e-02])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(3.0842692852020264, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(3.1638472080230713, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0334345661103725, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.5388651490211487, 0.4498935937881470, 0.0082421908155084])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0.1238896995782852, 0.3105658888816833, 0.0785723775625229])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([1.0580972582101822e-01, 8.8603538274765015e-01, 8.3327558968449011e-06,\n",
      "        2.7513194084167480e-01, 2.9082128405570984e-01])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([1.5500795841217041e-01, 7.4058698373846710e-07, 1.8764820694923401e-01,\n",
      "        4.1558127850294113e-02, 2.3817044496536255e-01])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(2.9389517307281494, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(3.0303733348846436, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0249962378293276, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.2653443217277527, 0.5245402455329895, 0.0578488782048225])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0.4000467061996460, 0.3856983780860901, 0.2399608492851257])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([9.7126469016075134e-02, 8.3473479747772217e-01, 5.9569638324319385e-06,\n",
      "        4.1547450423240662e-01, 1.5214703977108002e-01])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([1.7165182530879974e-01, 1.2922497489853413e-06, 1.9146661460399628e-01,\n",
      "        5.5942632257938385e-02, 5.6974041461944580e-01])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(2.7995624542236328, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(2.9018614292144775, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0186810325831175, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.3351887464523315, 0.6540845632553101, 0.0496927648782730])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0.5031021237373352, 0.4147555828094482, 0.1384263336658478])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.3333333432674408)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(0.5000000000000000)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([9.6202403306961060e-02, 7.3408830165863037e-01, 1.9632409475889290e-06,\n",
      "        5.5774641036987305e-01, 7.5815930962562561e-02])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([3.7354445457458496e-01, 3.2834573175932746e-06, 1.2208266556262970e-01,\n",
      "        6.3174888491630554e-02, 8.6892277002334595e-01])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(2.6646471023559570, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(2.7762818336486816, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0173677839338779, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.3807071149349213, 0.6020470857620239, 0.0572001710534096])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([0.5830835103988647, 0.4872848689556122, 0.0708219110965729])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.3333333432674408)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(0.5000000000000000)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([9.7194850444793701e-02, 5.9354698657989502e-01, 1.8507339518691879e-06,\n",
      "        6.0651910305023193e-01, 4.2992629110813141e-02])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([4.2861485481262207e-01, 3.7026488826086279e-06, 1.3399375975131989e-01,\n",
      "        6.9128170609474182e-02, 8.8460683822631836e-01])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align_mean tensor(nan)\n",
      "evaluation_human_metric_hard_accuracy tensor(0.)\n",
      "evaluation_human_metric_hard_f1 tensor(nan)\n",
      "evaluation_mouse_metric_hard_accuracy_1 tensor(0.)\n",
      "evaluation_mouse_metric_hard_f1_1 tensor(nan)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Current Batch:'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key mouse_cond_kl_loss\n",
      "val tensor(2.5379054546356201, grad_fn=<MeanBackward0>)\n",
      "key human_cond_kl_loss\n",
      "val tensor(2.6552693843841553, grad_fn=<MeanBackward0>)\n",
      "key align_cond_kl_loss\n",
      "val tensor(0.0154118137434125, grad_fn=<MulBackward0>)\n",
      "A_given_B: tensor([[ 2, 11],\n",
      "        [ 4, 13],\n",
      "        [ 9, 17]]) tensor([0.3611454367637634, 0.6615246534347534, 0.3944798409938812])\n",
      "B_given_A: tensor([[11,  2],\n",
      "        [13,  4],\n",
      "        [17,  9]]) tensor([6.2440097332000732e-01, 5.8444190025329590e-01, 2.0779941807977500e-38])\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align tensor(0.3333333432674408)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align tensor(0.5000000000000000)\n",
      "align_evaluation_train_align_0.5_metric_hard_accuracy_align_mean tensor(0.3333333432674408)\n",
      "align_evaluation_train_align_0.5_metric_hard_f1_align_mean tensor(0.5000000000000000)\n",
      "A_given_B: tensor([[ 6, 15],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 15],\n",
      "        [ 6, 11]]) tensor([8.5238732397556305e-02, 3.9773839712142944e-01, 4.6099812607280910e-06,\n",
      "        6.2120318412780762e-01, 2.7814995497465134e-02])\n",
      "B_given_A: tensor([[15,  6],\n",
      "        [16,  6],\n",
      "        [15,  5],\n",
      "        [15,  4],\n",
      "        [11,  6]]) tensor([2.8062051534652710e-01, 2.0229720121278660e-06, 1.8934968113899231e-01,\n",
      "        5.6302838027477264e-02, 8.0353790521621704e-01])\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align tensor(0.8000000119209290)\n",
      "align_evaluation_dev_align_0.5_metric_hard_f1_align tensor(nan)\n",
      "align_evaluation_dev_align_0.5_metric_hard_accuracy_align_mean tensor(0.8000000119209290)\n",
      "\n",
      "Stopped training at 10.0 epochs due to keyboard interrupt.\n"
     ]
    }
   ],
   "source": [
    "nEpochs = 1000\n",
    "l.train(nEpochs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
